{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8209071,"sourceType":"datasetVersion","datasetId":4864577}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nimport os\nimport torch\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FMNIST Classses\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#installing FMNIST dataset\nfashion_mnist = keras.datasets.fashion_mnist\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\nX_train_full.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalizing the data\nX_train_full = X_train_full / 255.0\nX_test = X_test / 255.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\n#convert the labels to one-hot encoding\ny_train = keras.utils.to_categorical(y_train, 10)\ny_valid = keras.utils.to_categorical(y_valid, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\ny_train_full = keras.utils.to_categorical(y_train_full, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_array_to_rgb(images):\n    # Create an empty list to store converted RGB images\n    rgb_images = []\n    \n    # Iterate over each image in the array\n    for image in images:\n        # Duplicate the single channel to create three channels\n        rgb_image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n        rgb_images.append(rgb_image)\n    \n    # Convert the list of RGB images to a NumPy array\n    rgb_images = np.array(rgb_images)\n    \n    return rgb_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_rgb = convert_array_to_rgb(X_train)\nX_valid_rgb = convert_array_to_rgb(X_valid)\nX_test_rgb = convert_array_to_rgb(X_test)\nX_train_full_rgb = convert_array_to_rgb(X_train_full)\n\nX_train_rgb.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CustomCNN(nn.Module):\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm1 = nn.BatchNorm2d(64)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm2 = nn.BatchNorm2d(128)\n\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm3 = nn.BatchNorm2d(256)\n\n        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=1)  # default stride is 2\n        self.batch_norm4 = nn.BatchNorm2d(512)\n\n        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=1)  # default stride is 2\n        self.batch_norm5 = nn.BatchNorm2d(512)\n\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(512, 4096)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(4096, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.pool1(x)\n        x = self.batch_norm1(x)\n\n        x = torch.relu(self.conv3(x))\n        x = torch.relu(self.conv4(x))\n        x = self.pool2(x)\n        x = self.batch_norm2(x)\n\n        x = torch.relu(self.conv5(x))\n        x = torch.relu(self.conv6(x))\n        x = torch.relu(self.conv7(x))\n        x = self.pool3(x)\n        x = self.batch_norm3(x)\n\n        x = torch.relu(self.conv8(x))\n        x = torch.relu(self.conv9(x))\n        x = torch.relu(self.conv10(x))\n        x = self.pool4(x)\n        x = self.batch_norm4(x)\n\n        x = torch.relu(self.conv11(x))\n        x = torch.relu(self.conv12(x))\n        x = torch.relu(self.conv13(x))\n        x = self.pool5(x)\n        x = self.batch_norm5(x)\n\n        x = self.flatten(x)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        features = torch.clone(x.cpu().detach())\n        x = self.fc3(x)\n        x = F.softmax(x, dim = 1)\n        return x, features\n\n# Create an instance of the model\nmodel = CustomCNN()\nprint(model)\n# model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CustomCNN()\nmodel.load_state_dict(torch.load('/kaggle/input/keytiti/VGG16.pt', map_location=torch.device('cpu')))\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nloss_object = nn.CrossEntropyLoss()\n\ndef create_adversarial_pattern(input_image, input_label, model):\n    # Create a new tensor with requires_grad enabled\n    input_image = input_image.clone().detach().requires_grad_(True)\n    \n    # Forward pass\n    prediction = model(input_image)\n    \n    # If the model returns a tuple, extract the logits\n    if isinstance(prediction, tuple):\n        prediction = prediction[0]  # Assuming the logits are the first element\n    \n    # Calculate loss\n    loss = loss_object(prediction, input_label.argmax(dim=1))\n    \n    # Backpropagation to compute gradients\n    model.zero_grad()\n    loss.backward()\n    \n    # Get the gradient of the input image\n    gradient = input_image.grad.data\n    \n    # Convert the gradient to signed gradients\n    signed_grad = torch.sign(gradient)\n    \n    return signed_grad\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move input tensors to the same device as the model\nimage = image.to(device)\nimage_label = image_label.to(device)\n\n# Perturbations\nperturbations = create_adversarial_pattern(image, image_label, model)\n\n# Print original class prediction\nimage_pred = model(image)\n\nprint(\"Original class prediction:\", image_pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Move perturbations to CPU before visualization\nperturbations_cpu = perturbations.cpu()\n\n# Show the perturbations\nplt.imshow(perturbations_cpu[0].permute(1, 2, 0).numpy() * 0.5 + 0.5)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the adversarial image\ndef display_images(image, description):\n    plt.figure()\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(description)\n    plt.show()\n    \ndef adversarial_pattern(image, label):\n    perturbations = create_adversarial_pattern(image, label, model)\n    return perturbations\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perturbations = adversarial_pattern(image, image_label)\nadv_x = image + perturbations * 0.1\n\n# Assuming adversarial_pattern, display_images functions are defined elsewhere\n\n# Ensure adv_x is on CPU and convert it to a numpy array\nadv_x_cpu = adv_x.cpu().detach().numpy()\n\n# Clip the values between 0 and 1\nadv_x_cpu = np.clip(adv_x_cpu, 0, 1)\n\n\nadv_image = adv_x_cpu[0, 0]  \n\nplt.figure()\nplt.axis('off')\nplt.imshow(adv_image, cmap='gray')  # Assuming grayscale image, so specifying cmap='gray'\nplt.title('Adversarial Image')\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting the adversarial image\nadv_pred = model(adv_x)\nadv_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\n\n# Define the device (CUDA or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to generate adversarial examples for a dataset\ndef generate_adversarial_dataset(X, y, model, epsilon=0.1):\n    # Set the model to evaluation mode\n    model.eval()\n    \n    adversarial_dataset = []\n    \n    for i in tqdm(range(len(X))):\n        # Convert image and label to PyTorch tensors\n        image = torch.tensor(X[i], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n        image_label = torch.tensor([y[i]]).to(device)  # Move label to the device\n\n        # Generate perturbations\n        perturbations = adversarial_pattern(image, image_label)\n        \n        # Create adversarial example\n        adv_x = image + perturbations * epsilon\n        \n        # Ensure adv_x is on CPU and convert it to a numpy array\n        adv_x_cpu = adv_x.cpu().detach().numpy()\n        \n        # Clip the values between 0 and 1\n        adv_x_cpu = np.clip(adv_x_cpu, 0, 1)\n        \n        # Store adversarial example and label\n        adversarial_dataset.append((adv_x_cpu.squeeze(0), y[i]))  # Remove batch dimension before storing\n        \n    return adversarial_dataset\n\n# Generate adversarial dataset for training set\nadversarial_train_set = generate_adversarial_dataset(X_train_full, y_train_full, model)\n\n# Generate adversarial dataset for test set\nadversarial_test_set = generate_adversarial_dataset(X_test, y_test, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}